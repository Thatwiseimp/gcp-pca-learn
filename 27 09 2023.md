# 27/09/2023

Diagnostic question — Cymbal Direct developers have written a new application. Based on initial usage estimates, you decide to run the application on Compute Engine instances with 15 Gb of RAM and 4 CPUs. These instances store persistent data locally. After the application runs for several months, historical data indicates that the application requires 30 Gb of RAM. The team wants you to make adjustments to minimize costs

Responses

- change machine type to e2-standard-8
- change machine type to e2-standard-8 with pre-emptible: True
- change machine type to custom-4-30720
- change machine type to custom-4-30720 with pre-emptible: True

Having said the machines store persistent data locally, we cant have them be deleted when the demand arises. Hence pre-emptible cannot be used in this case. Having e2 standard 8 with higher vCPU, it might incur higher costs. Hence by the rule of elimination, we eliminate A,B and D leaving choice C the answer

![Screenshot 2023-09-27 at 6.00.07 PM.png](27%2009%202023%204cf474f4811c4c4283c11cf834228dc6/Screenshot_2023-09-27_at_6.00.07_PM.png)

Diagnostic question —  You are creating a new project. You plan to set up a Dedicated interconnect between two of your data centers and want to ensure that your resources are only deployed to the same regions where your data centers are located. You need to make sure that you don’t have any overlapping IP addresses that could cause conflicts when you set up the interconnect. You want to use RFC 1918 (nineteen - eighteen) class B address space.

*An RFC1918 address is an IP address that is assigned by an enterprise organization to an internal host. These IP addresses are used in private networks, which are not available, or reachable, from the Internet. In fact, one of the basic requirements of the Internet is that each host/machine has a unique IP address. RFC1918 removes this requirement. RFC1918 IP addresses can be used on multiple networks (repetitive across different network blocks), as long as they’re private and isolated from each other. To implement this solution every Internet router must be configured to discard IP packets with these addresses. IP packets carrying private addresses can only flow on internal, private networks. This aspect contributes to network security by creating a clear distinction between internal and external networks.*

![Screenshot 2023-09-27 at 6.15.26 PM.png](27%2009%202023%204cf474f4811c4c4283c11cf834228dc6/Screenshot_2023-09-27_at_6.15.26_PM.png)

The address spaces are the possible 

*The RFC1918 subnets are always paired with NAT. NAT enables the internal host communicate with the internet. It has two interfaces, the internet interface — configured with a public IP address and a private interface — connected to the internal network and is configured with a private IP (RFC1918) address. When the NAT device receives a packet from an internal host, it rewrites the packet using its own public IP address as source before sending it to the Internet. This process is also called “masquerading” because it seems as if the conversation was (falsely) originated by the NAT device itself.*

**ExamTopics Questions:**

You write a Python script to connect to Google BigQuery from a Google Compute Engine virtual machine. The script is printing errors that it cannot connect to BigQuery. What should you do to fix the script?

- A. Install the latest BigQuery API client library for Python **[Most Voted]**
- B. Run your script on a new virtual machine with the BigQuery access scope enabled
- C. Create a new service account with BigQuery access and execute your script with that user  **[Most Voted]**
- D. Install the bq component for gcloud with the command gcloud components install bq.

Response: 

A - If client library was not installed, the python scripts won't run - since the question states the script reports "cannot connect" - the client library must have been installed. so it's B or C.

B - https://cloud.google.com/bigquery/docs/authorization an access scope is how your client application retrieve access_token with access permission in OAuth when you want to access services via API call - in this case, it is possible that the python script use an API call instead of library, if this is true, then access scope is required. client library requires no access scope (as it does not go through OAuth)

C - service account is Google Cloud's best practice
So prefer C.

---

Your customer is moving an existing corporate application to Google Cloud Platform from an on-premises data center. The business owners require minimal user disruption. There are strict security team requirements for storing passwords.

What authentication strategy should they use?

- A. Use G Suite Password Sync to replicate passwords into Google
- B. Federate authentication via SAML 2.0 to the existing Identity Provider **[Most Voted]**
- C. Provision users in Google using the Google Cloud Directory Sync tool
- D. Ask users to set their Google password to match their corporate password

Response:

I think it's B because they want minimal user disruption, and only this option focuses on using the same password. Plus, they want to move ONE existing corporate application, not all their infrastructure.

A. I don't think this meets a strict security requirement, and if they eventually need to change the password, I think this would not be synced or may have issues syncing both passwords.
C. We don't want to provision new users; we want to keep users with minimal disruption and doing what they do already taking the least steps possible.
D. Probably a terrible security practice; if anything, we would like them to use one password and sign in from there.

B seems to me the most fitting.

*Federate authentication via SAML 2.0 to the existing Identity Provider - Federated authentication allows users to sign in to the Google Cloud Platform using the same credentials they use for their corporate accounts. It delegates the authentication process to an existing Identity Provider (IdP) that the company uses on-premises. This approach minimizes user disruption, as users don't have to remember a separate set of credentials for Google Cloud, and it allows the company to maintain its existing security policies and password storage requirements.*